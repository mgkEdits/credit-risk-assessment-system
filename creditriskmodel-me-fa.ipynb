{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12895455,"sourceType":"datasetVersion","datasetId":8159045}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/elijahnyasiando/creditriskmodel-me-fa?scriptVersionId=290095208\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Modules","metadata":{}},{"cell_type":"code","source":"import warnings\n# warnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, make_scorer\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Set some display options for better viewing\npd.set_option('display.max_columns', 50)\nsns.set_style('whitegrid')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:48:27.915858Z","iopub.execute_input":"2025-12-19T06:48:27.916143Z","iopub.status.idle":"2025-12-19T06:48:31.542472Z","shell.execute_reply.started":"2025-12-19T06:48:27.916122Z","shell.execute_reply":"2025-12-19T06:48:31.541593Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating Master Training Dataset","metadata":{}},{"cell_type":"code","source":"COHORTS = ['2018Q1','2018Q2','2018Q3','2018Q4', '2019Q1', '2019Q2', '2019Q3', '2019Q4']\n\nORIGINATION_COL_NAMES = [\n    'CREDIT_SCORE', 'FIRST_PAYMENT_DATE', 'FIRST_TIME_HOMEBUYER_FLAG',\n    'MATURITY_DATE', 'METROPOLITAN_STATISTICAL_AREA',\n    'MORTGAGE_INSURANCE_PERCENTAGE', 'NUMBER_OF_UNITS', 'OCCUPANCY_STATUS',\n    'ORIGINAL_COMBINED_LOAN_TO_VALUE_CLTV', 'ORIGINAL_DEBT_TO_INCOME_DTI_RATIO',\n    'ORIGINAL_UPB', 'ORIGINAL_LOAN_TO_VALUE_LTV', 'ORIGINAL_INTEREST_RATE',\n    'CHANNEL', 'PREPAYMENT_PENALTY_MORTGAGE_FLAG', 'AMORTIZATION_TYPE',\n    'PROPERTY_STATE', 'PROPERTY_TYPE', 'POSTAL_CODE', 'LOAN_SEQUENCE_NUMBER',\n    'LOAN_PURPOSE', 'ORIGINAL_LOAN_TERM', 'NUMBER_OF_BORROWERS', 'SELLER_NAME',\n    'SERVICER_NAME', 'SUPER_CONFORMING_FLAG', 'PRE-RELIEF_REFINANCE_LOAN_SEQUENCE_NUMBER',\n    'SPECIAL_ELIGIBILITY_PROGRAM', 'RELIEF_REFINANCE_INDICATOR', 'PROPERTY_VALUATION_METHOD',\n    'INTEREST_ONLY_INDICATOR', 'MI_CANCELLATION_INDICATOR'\n]\n\nPERFORMANCE_COL_NAMES = [\n    'LOAN_SEQUENCE_NUMBER', 'MONTHLY_REPORTING_PERIOD', 'CURRENT_ACTUAL_UPB',\n    'CURRENT_LOAN_DELINQUENCY_STATUS', 'LOAN_AGE', 'REMAINING_MONTHS_TO_LEGAL_MATURITY',\n    'DEFECT_SETTLEMENT_DATE', 'MODIFICATION_FLAG', 'ZERO_BALANCE_CODE',\n    'ZERO_BALANCE_EFFECTIVE_DATE', 'CURRENT_INTEREST_RATE', 'CURRENT_NON-INTEREST_BEARING_UPB',\n    'DUE_DATE_OF_LAST_PAID_INSTALLMENT_DDLPI', 'MI_RECOVERIES', 'NET_SALE_PROCEEDS',\n    'NON_MI_RECOVERIES', 'TOTAL_EXPENSES', 'LEGAL_COSTS',\n    'MAINTENANCE_AND_PRESERVATION_COSTS', 'TAXES_AND_INSURANCE', 'MISCELLANEOUS_EXPENSES',\n    'ACTUAL_LOSS_CALCULATION', 'CUMULATIVE_MODIFICATION_COST', 'STEP_MODIFICATION_FLAG',\n    'PAYMENT_DEFERRAL', 'ESTIMATED_LOAN_TO_VALUE_ELTV', 'ZERO_BALANCE_REMOVAL_UPB',\n    'DELINQUENT_ACCRUED_INTEREST', 'DELINQUENCY_DUE_TO_DISASTER',\n    'BORROWER_ASSISTANCE_STATUS_CODE', 'CURRENT_MONTH_MODIFICATION_COST', 'INTEREST_BEARING_UPB'\n]\n\nCOVARIATES_TO_KEEP = [\n    'LOAN_SEQUENCE_NUMBER', 'CREDIT_SCORE', 'ORIGINAL_DEBT_TO_INCOME_DTI_RATIO',\n    'ORIGINAL_LOAN_TO_VALUE_LTV', 'ORIGINAL_UPB', 'ORIGINAL_INTEREST_RATE',\n    'NUMBER_OF_UNITS', 'NUMBER_OF_BORROWERS', 'LOAN_PURPOSE', 'CHANNEL','FIRST_TIME_HOMEBUYER_FLAG', 'OCCUPANCY_STATUS', 'PROPERTY_TYPE'\n]\n\nperf_dtypes = {\n    'LOAN_SEQUENCE_NUMBER': 'string',\n    'CURRENT_LOAN_DELINQUENCY_STATUS': 'string', # Keep string, handle 'XX'/'R' later\n    'ZERO_BALANCE_CODE': 'string'\n}\n\nall_cohort_data = []\n\nfor cohort in COHORTS:\n    print(f\"--> Processing cohort: {cohort}...\")\n    \n    # --- OPTIMIZATION: Process Performance Data in Chunks ---\n    perf_file_path = f'/kaggle/input/2018-2019-small-loans-data/data/historical_data_time_{cohort}.txt'\n    \n    chunk_list = []\n    # Read in chunks of 1 million rows to prevent OOM\n    for chunk in pd.read_csv(perf_file_path, sep='|', header=None, names=PERFORMANCE_COL_NAMES,\n                             usecols=['LOAN_SEQUENCE_NUMBER', 'CURRENT_LOAN_DELINQUENCY_STATUS', 'ZERO_BALANCE_CODE'],\n                             dtype=perf_dtypes, chunksize=1_000_000):\n        \n        # Vectorized clean-up within chunk\n        chunk['delinq_num'] = pd.to_numeric(chunk['CURRENT_LOAN_DELINQUENCY_STATUS'], errors='coerce').fillna(0)\n        \n        # Vectorized Flagging\n        # Flag 1: Every 90+ DPD\n        chunk['is_90plus'] = (chunk['delinq_num'] >= 3)\n        # Flag 2: Bad Termination (03=Foreclosure, 09=Deed/Short Sale)\n        chunk['is_bad_term'] = chunk['ZERO_BALANCE_CODE'].isin(['03', '09'])\n        \n        # Group by Loan ID within chunk? NO. Loans span chunks. \n        # We must filter to relevant rows or aggregate partially.\n        # STRATEGY: Reduce chunk to only \"Bad\" events, then aggregate later.\n        \n        # Filter chunk to only rows that indicate a default event\n        bad_events = chunk[(chunk['is_90plus']) | (chunk['is_bad_term'])][['LOAN_SEQUENCE_NUMBER']]\n        chunk_list.append(bad_events)\n\n    # Concatenate all \"Bad\" events found\n    all_bad_loans = pd.concat(chunk_list)\n    \n    # Get unique set of Defaulters\n    defaulters_set = set(all_bad_loans['LOAN_SEQUENCE_NUMBER'].unique())\n    print(f\"    Identified {len(defaulters_set)} unique defaulters in {cohort}.\")\n\n    # --- Process Origination Data ---\n    print(f\"    Reading origination data for {cohort}...\")\n    orig_file_path = f'/kaggle/input/2018-2019-small-loans-data/data/historical_data_{cohort}.txt'\n    \n    # Load Origination (Usually fits in memory, but use dtypes to be safe)\n    df_orig = pd.read_csv(orig_file_path, sep='|', header=None, names=ORIGINATION_COL_NAMES,\n                          usecols=COVARIATES_TO_KEEP, dtype={'LOAN_SEQUENCE_NUMBER': 'string'})\n\n    # --- Map Targets ---\n    # 1 if in defaulters_set, else 0\n    df_orig['Default_Flag'] = df_orig['LOAN_SEQUENCE_NUMBER'].isin(defaulters_set).astype(int)\n    \n    all_cohort_data.append(df_orig)\n\n# Final Concatenation\nmaster_training_set = pd.concat(all_cohort_data, ignore_index=True)\nmaster_training_set.to_parquet('master_training_set.parquet', index=False)\n\nprint(\"\\nPipeline complete! The file is ready for your modeling notebook.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T06:48:36.891164Z","iopub.execute_input":"2025-12-19T06:48:36.891641Z","iopub.status.idle":"2025-12-19T06:51:56.703718Z","shell.execute_reply.started":"2025-12-19T06:48:36.891615Z","shell.execute_reply":"2025-12-19T06:51:56.702756Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # Exploratory Data Analysis (EDA)\n Initial Overview","metadata":{}},{"cell_type":"code","source":"\n# --- CONFIGURATION ---\nINPUT_FILE = 'master_training_set.parquet'\nTARGET_COL = 'Default_Flag'\n\n# Regulatory Whitelist: Features strictly related to Capacity (Income) & Collateral (Asset)\n# This excludes 'Postal Code' or 'MSA' to minimize geographic bias (Redlining risks)\nNUMERIC_FEATURES = [\n    'CREDIT_SCORE', \n    'ORIGINAL_DEBT_TO_INCOME_DTI_RATIO', \n    'ORIGINAL_LOAN_TO_VALUE_LTV', \n    'ORIGINAL_UPB', \n    'ORIGINAL_INTEREST_RATE'\n]\n\nCATEGORICAL_FEATURES = [\n    'CHANNEL', \n    'LOAN_PURPOSE', \n    'FIRST_TIME_HOMEBUYER_FLAG', \n    'OCCUPANCY_STATUS', \n    'PROPERTY_TYPE'\n]\n\ndef load_and_clean_data(file_path):\n    \"\"\"\n    Loads data and applies Engineering-Driven Cleaning logic.\n    \"\"\"\n    print(f\"--> Loading dataset from {file_path}...\")\n    df = pd.read_parquet(file_path)\n    \n    # 1. MSA Logic: Treat Null as 'Rural/Unknown' (Kenya DPA: Avoid bias against unmapped areas)\n    if 'METROPOLITAN_STATISTICAL_AREA' in df.columns:\n        df['METROPOLITAN_STATISTICAL_AREA'] = df['METROPOLITAN_STATISTICAL_AREA'].fillna('Rural_Unknown')\n\n    # 2. Credit Score Logic: Remove Invalid Data\n    # 9999 is often used as a legacy error code in credit bureaus. \n    # Scores < 300 are theoretically impossible in standard FICO.\n    print(\"    Cleaning Credit Scores...\")\n    df = df[df['CREDIT_SCORE'] != 9999] \n    df = df[df['CREDIT_SCORE'] > 300]\n    \n    # 3. DTI Logic: The \"Hidden Risk\" Imputation\n    # We do NOT just fill with mean. Missing DTI often implies self-employment or non-conforming loans.\n    # We impute the median to preserve distribution, but create a binary flag to capture the risk.\n    if df['ORIGINAL_DEBT_TO_INCOME_DTI_RATIO'].isnull().sum() > 0:\n        print(\"    Imputing missing DTI and creating flagging feature...\")\n        df['DTI_MISSING_FLAG'] = df['ORIGINAL_DEBT_TO_INCOME_DTI_RATIO'].isnull().astype(int)\n        df['ORIGINAL_DEBT_TO_INCOME_DTI_RATIO'] = df['ORIGINAL_DEBT_TO_INCOME_DTI_RATIO'].fillna(\n            df['ORIGINAL_DEBT_TO_INCOME_DTI_RATIO'].median()\n        )\n    \n    # 4. LTV Cleaning\n    df['ORIGINAL_LOAN_TO_VALUE_LTV'] = df['ORIGINAL_LOAN_TO_VALUE_LTV'].fillna(\n        df['ORIGINAL_LOAN_TO_VALUE_LTV'].median()\n    )\n\n    print(f\"    Data Cleaned. Rows remaining: {len(df)}\")\n    return df\n\ndef analyze_monotonicity(df):\n    \"\"\"\n    Checks if risk drivers behave logically (Monotonic trends).\n    This is required for Explainability (Right to Explanation).\n    \"\"\"\n    print(\"\\n--> 1. Checking Monotonicity (Risk Banding)...\")\n    \n    # Binning FICO Scores\n    # Standard Risk Bands: Subprime (<620), Near Prime (620-660), Prime (660-720), Super Prime (720+)\n    bins = [300, 620, 660, 700, 740, 780, 850]\n    labels = ['<620', '620-660', '660-700', '700-740', '740-780', '780+']\n    \n    df['FICO_Band'] = pd.cut(df['CREDIT_SCORE'], bins=bins, labels=labels)\n    \n    # Calculate Default Rate per Band\n    risk_profile = df.groupby('FICO_Band', observed=True)[TARGET_COL].mean().reset_index()\n    risk_profile.rename(columns={TARGET_COL: 'Default_Rate'}, inplace=True)\n    \n    # Visualization\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x='FICO_Band', y='Default_Rate', data=risk_profile, palette='Reds_r')\n    plt.title(\"Monotonicity Check: Default Rate by Credit Score Band\")\n    plt.ylabel(\"Probability of Default (PD)\")\n    plt.grid(axis='y', alpha=0.3)\n    plt.show()\n    \n    print(\"    *Architect Note*: If the bars do not descend like a staircase, the model will be unstable.\")\n\ndef visualize_tipping_points(df):\n    \"\"\"\n    Identifies non-linear 'Cliffs' where risk spikes.\n    \"\"\"\n    print(\"\\n--> 2. Visualizing Non-Linear Risk Cliffs (Capacity & Collateral)...\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n    \n    # Plot A: DTI (Capacity)\n    # We look for the '43%' QM Rule cliff\n    sns.kdeplot(data=df[df[TARGET_COL]==0], x='ORIGINAL_DEBT_TO_INCOME_DTI_RATIO', \n                color='green', label='Good Loans', ax=axes[0], fill=True, alpha=0.1)\n    sns.kdeplot(data=df[df[TARGET_COL]==1], x='ORIGINAL_DEBT_TO_INCOME_DTI_RATIO', \n                color='red', label='Defaulters', ax=axes[0], fill=True, alpha=0.1)\n    \n    axes[0].set_title(\"Capacity Constraint: DTI Distribution\")\n    axes[0].set_xlim(10, 60)\n    axes[0].axvline(43, color='black', linestyle='--', label='QM Threshold (43%)')\n    axes[0].legend()\n\n    # Plot B: LTV (Collateral)\n    # We look for the '80%' PMI cliff\n    sns.kdeplot(data=df[df[TARGET_COL]==0], x='ORIGINAL_LOAN_TO_VALUE_LTV', \n                color='green', label='Good Loans', ax=axes[1], fill=True, alpha=0.1)\n    sns.kdeplot(data=df[df[TARGET_COL]==1], x='ORIGINAL_LOAN_TO_VALUE_LTV', \n                color='red', label='Defaulters', ax=axes[1], fill=True, alpha=0.1)\n    \n    axes[1].set_title(\"Collateral Risk: LTV Distribution\")\n    axes[1].set_xlim(50, 105)\n    axes[1].axvline(80, color='black', linestyle='--', label='PMI Threshold (80%)')\n    axes[1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n\ndef correlation_matrix(df):\n    \"\"\"\n    Simple linear correlation check.\n    \"\"\"\n    print(\"\\n--> 3. Correlation Matrix...\")\n    cols = NUMERIC_FEATURES + [TARGET_COL]\n    corr = df[cols].corr()\n    \n    plt.figure(figsize=(8, 6))\n    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title(\"Correlation of Risk Drivers\")\n    plt.show()\n\ndef finalize_features(df):\n    \"\"\"\n    Selects only whitelist features for the modeling stage.\n    \"\"\"\n    print(\"\\n--> 4. Final Feature Selection...\")\n    \n    # Check for engineered features\n    final_numeric = NUMERIC_FEATURES.copy()\n    if 'DTI_MISSING_FLAG' in df.columns:\n        final_numeric.append('DTI_MISSING_FLAG')\n        \n    # Filter Dataset\n    final_cols = final_numeric + CATEGORICAL_FEATURES + [TARGET_COL]\n    df_final = df[final_cols].copy()\n    \n    print(f\"    Selected {len(final_numeric)} Numeric & {len(CATEGORICAL_FEATURES)} Categorical features.\")\n    print(\"    Ready for Weight of Evidence (WoE) Transformation.\")\n    \n    return df_final\n\n# --- EXECUTION PIPELINE ---\ndf_master = load_and_clean_data(INPUT_FILE)\nanalyze_monotonicity(df_master)\nvisualize_tipping_points(df_master)\ncorrelation_matrix(df_master)\ndf_model_ready = finalize_features(df_master)\n\n# Display first few rows of the clean dataset\ndf_model_ready.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T07:00:38.973066Z","iopub.execute_input":"2025-12-19T07:00:38.973707Z","iopub.status.idle":"2025-12-19T07:01:09.040359Z","shell.execute_reply.started":"2025-12-19T07:00:38.973678Z","shell.execute_reply":"2025-12-19T07:01:09.039589Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Engineering and Financial Calibration\n\n## 1. Architectural Objective\nThe objective of this phase was to transition from exploratory analysis to a production-grade Probability of Default (PD) classifier. Unlike standard machine learning tasks where \"Accuracy\" is the primary metric, this architecture prioritizes Financial Calibration and Regulatory Stability. The system is designed to minimize the economic impact of loan losses while adhering to Basel III standards for internal models.\n\n## 2. Core Methodology: Cost-Sensitive Random Forest\nWe selected a Random Forest ensemble architecture due to its ability to capture non-linear interactions between key risk drivers (e.g., the compounding risk of low FICO scores combined with high LTV ratios) without requiring complex feature transformation.\n\n### Key Engineering Decisions:\n\n#### Data Integrity (No SMOTE):\nWe explicitly rejected synthetic oversampling techniques (such as SMOTE). Creating \"fake\" synthetic borrowers to balance classes introduces statistical noise and violates the Data Integrity principles required for auditability. Instead, we utilized the model's native class_weight='balanced' parameter to mathematically penalize the misclassification of defaulters.\n#### Stability Constraints:\nTo prevent the model from learning \"anecdotal\" noise, we enforced a strict min_samples_leaf=50 constraint. This ensures that every decision rule (branch) in the forest is supported by a cohort of at least 50 actual historical loans, guaranteeing that our risk assessments are statistically significant and robust.\n#### Stratified Validation:\nAll training and validation utilized Stratified K-Fold Cross-Validation to strictly maintain the population's natural default rate (approx. 1.5%) across all testing folds, preventing \"lucky\" splits that could mask model weakness.\n\n## 3. Financial Calibration (The \"Custom Scorer\")\nA generic model treats a False Positive (rejecting a good customer) and a False Negative (approving a defaulter) as equal errors. In banking, this is false.\nWe engineered a Custom Loss Function for the Hyperparameter Tuning grid:\n\nCost of False Negative ($10): Represents the Principal Loss (LGD) and workout costs.\n\nCost of False Positive ($1): Represents the Opportunity Cost (lost interest income).\n\nThe model optimization process (GridSearchCV) was directed to minimize this weighted financial loss rather than maximizing raw accuracy.\n\n## 4. Regulatory Compliance & Ethics\nKenya Data Protection Act (2019): By using a tree-based model without \"black-box\" neural networks or synthetic data, we ensure the \"Right to Explanation.\" Every prediction can be traced back to specific, intelligible financial thresholds (e.g., \"DTI > 43%\").\nFair Lending: The feature set was restricted strictly to Financial Capacity and Collateral metrics, explicitly excluding geographic and demographic proxies to prevent redlining or bias.","metadata":{}},{"cell_type":"code","source":"# --- CONFIGURATION ---\nTARGET_COL = 'Default_Flag'\n\n# \"Financial Calibration\" Constants\nCOST_FALSE_NEGATIVE = 10  # Unit cost of Default (Loss Given Default)\nCOST_FALSE_POSITIVE = 1   # Unit cost of Rejection (Lost Interest Income)\n\n# Feature Lists\nNUMERIC_FEATURES = [\n    'CREDIT_SCORE', \n    'ORIGINAL_DEBT_TO_INCOME_DTI_RATIO', \n    'ORIGINAL_LOAN_TO_VALUE_LTV', \n    'ORIGINAL_UPB', \n    'ORIGINAL_INTEREST_RATE',\n    'NUMBER_OF_UNITS',\n    'NUMBER_OF_BORROWERS'\n]\n\nCATEGORICAL_FEATURES = [\n    'CHANNEL', \n    'LOAN_PURPOSE'\n]\n\ndef custom_financial_loss_score(y_true, y_pred):\n    \"\"\"\n    Custom Scorer for GridSearchCV.\n    Optimizes for Lowest Financial Loss rather than raw Accuracy.\n    \"\"\"\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    \n    # Total Cost Calculation\n    total_cost = (fn * COST_FALSE_NEGATIVE) + (fp * COST_FALSE_POSITIVE)\n    \n    # We negate because GridSearchCV tries to maximize the score\n    return -total_cost\n\ndef load_data():\n    print(f\"--> Loading {INPUT_FILE}...\")\n    df = pd.read_parquet(INPUT_FILE)\n    \n    # Infinite/NaN hygiene\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.dropna(subset=NUMERIC_FEATURES, inplace=True)\n    \n    X = df[NUMERIC_FEATURES + CATEGORICAL_FEATURES]\n    y = df[TARGET_COL]\n    return X, y\n\ndef build_pipeline():\n    \"\"\"\n    Constructs the processing pipeline without Synthetic Sampling.\n    \"\"\"\n    # 1. Preprocessing\n    numeric_transformer = StandardScaler()\n    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, NUMERIC_FEATURES),\n            ('cat', categorical_transformer, CATEGORICAL_FEATURES)\n        ]\n    )\n    \n    # 2. Classifier\n    # class_weight='balanced': Adjusts weights inversely proportional to class frequencies.\n    # This is the \"Regulatory Compliant\" way to handle imbalance.\n    rf = RandomForestClassifier(\n        class_weight='balanced',\n        random_state=42,\n        n_jobs=-1\n    )\n    \n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('classifier', rf)\n    ])\n    \n    return pipeline\n\ndef execute_training_grid(X_train, y_train):\n    print(\"\\n--> Starting Hyperparameter Tuning (GridSearchCV)...\")\n    print(\"    Prioritizing Model Stability (min_samples_leaf) over Granularity.\")\n\n    pipeline = build_pipeline()\n    \n    # --- The Grid ---\n    # max_depth: Restrict depth to prevent memorizing noise.\n    # min_samples_leaf: High values (50+) ensure every \"rule\" applies to a large cohort (Stability).\n    param_grid = {\n        'classifier__n_estimators': [100], # Keep constant for speed\n        'classifier__max_depth': [8, 12],\n        'classifier__min_samples_leaf': [50, 100] \n    }\n    \n    # --- Stratified Validation ---\n    # Ensures every fold has the same 1.5% default rate as the population.\n    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n    \n    # Use our Custom Financial Scorer\n    financial_scorer = make_scorer(custom_financial_loss_score)\n\n    grid_search = GridSearchCV(\n        pipeline,\n        param_grid,\n        cv=cv,\n        scoring=financial_scorer, # Optimize for Cost, not Accuracy\n        verbose=2,\n        n_jobs=-1\n    )\n    \n    grid_search.fit(X_train, y_train)\n    \n    print(f\"\\n    Best Parameters: {grid_search.best_params_}\")\n    return grid_search.best_estimator_\n\ndef evaluate_financial_impact(model, X_test, y_test):\n    print(\"\\n--> Generating Cost-Sensitive Evaluation...\")\n    \n    y_pred = model.predict(X_test)\n    \n    # 1. Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    \n    # 2. Cost Calculation\n    financial_loss = (fn * COST_FALSE_NEGATIVE) + (fp * COST_FALSE_POSITIVE)\n    \n    # 3. Report\n    print(\"\\n--- CONFUSION MATRIX (Raw Counts) ---\")\n    print(f\"True Negatives (Good Loans Approved): {tn}\")\n    print(f\"False Positives (Good Loans Rejected): {fp}  [Opp Cost: ${fp * COST_FALSE_POSITIVE}]\")\n    print(f\"False Negatives (Defaults Missed):     {fn}  [Risk Cost: ${fn * COST_FALSE_NEGATIVE}]\")\n    print(f\"True Positives (Defaults Caught):      {tp}\")\n    \n    print(\"\\n--- FINANCIAL PERFORMANCE ---\")\n    print(f\"Total Model Cost Score: {financial_loss}\")\n    print(f\"Ratio of Risk Cost vs Opp Cost: {(fn*COST_FALSE_NEGATIVE)/(fp*COST_FALSE_POSITIVE):.2f}x\")\n    \n    # 4. Visual Matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='RdBu', cbar=False)\n    plt.title(f\"Confusion Matrix\\n(Cost Penalty: FN=${COST_FALSE_NEGATIVE}, FP=${COST_FALSE_POSITIVE})\")\n    plt.ylabel('Actual Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n\n    # 5. Standard Metrics\n    print(\"\\n--- CLASSIFICATION REPORT ---\")\n    print(classification_report(y_test, y_pred))\n\n# --- MAIN EXECUTION ---\nif __name__ == \"__main__\":\n    # 1. Load\n    X, y = load_data()\n    \n    # 2. Split\n    # Stratified Split is mandatory for imbalanced datasets (Basel Requirement)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, stratify=y, random_state=42\n    )\n    \n    # 3. Tune & Train\n    best_model = execute_training_grid(X_train, y_train)\n    \n    # 4. Evaluate\n    evaluate_financial_impact(best_model, X_test, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T07:01:29.297025Z","iopub.execute_input":"2025-12-19T07:01:29.297615Z","iopub.status.idle":"2025-12-19T07:24:22.508868Z","shell.execute_reply.started":"2025-12-19T07:01:29.297588Z","shell.execute_reply":"2025-12-19T07:24:22.507579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The objective has been partially achieved, but the business viability is currently low. We`ve built a safety net, but it is too wide.\n\n### 1. E The \"Business Revolt\" Scenario\n\n*The Good News (Risk Safety):* The model achieved a Recall of 69%; successfully caught nearly 70% of all defaults (19,514 out of ~28k). In the world of unsampled, highly imbalanced data (1.5% default rate), this is a strong starting point for a Random Forest.\n\nThe Bad News (Commercial Viability): The Precision is 9%. This means for every 100 loans it rejected to prevent a default, 91 of them were actually good customers.\n##### The Verdict: The Business Unit (Sales/Origination) would reject this model. You are rejecting 197,484 good loans to save 8,627 defaults. While you avoided credit losses, you decimated the bank's revenue stream and market share.\n\n### 2. Deep Dive: Why did this happen?\n\nThe issue lies in the Financial Calibration (10:1 Cost Ratio) combined with class_weight='balanced'.\nDouble Penalty: The model used class_weight='balanced' (which mathematically boosts the minority class error) AND its optimized for a scorer that penalizes False Negatives 10x.\n\nThe Model's Logic: The model \"learned\" that the safest way to minimize cost is to aggressively label anything in the \"Grey Zone\" as a Default. It decided: \"It is cheaper to incorrectly reject a decent borrower (1cost)thantoriskapprovingabadone(1cost)thantoriskapprovingabadone(10 cost).\"\n\nThe Outcome: The model became extremely risk-averse. It successfully minimized the expensive errors (FN), but it accumulated so many cheap errors (FP) that the sheer volume of rejected interest income (197k)isnowdoublethesavedriskcost(197k)isnowdoublethesavedriskcost(86k).\n\n### 3. Next Steps: Refining the Model\n\nTo fix this, we must move from \"avoiding loss\" to \"optimizing profit.\" We need to improve Precision without destroying Recall.\n\n#### Step A: Probability Threshold Tuning (The \"Quick Win\")\nCurrently, the model uses a default threshold of 0.5 to decide Yes/No. Because of class_weight='balanced', the probabilities are skewed. The optimal decision boundary is likely much higher (e.g., 0.65 or 0.70).\n\n##### Action: Plot the Precision-Recall Curve. Find the threshold that yields Precision > 20% while keeping Recall > 50%.\nCode Implementation: Do not just predict classes (predict()); predict probabilities (predict_proba()) and apply a custom threshold.\n\n#### Step B: Interaction Features (The \"Signal Booster\")\nThe model is struggling to distinguish \"High Risk\" from \"Medium Risk\" (hence the low precision). We need sharper features.\n\n##### Action: Create Interaction Features to isolate specific toxic combinations.\nLTV_x_DTI: High debt + Low equity is worse than the sum of its parts.\nFICO_per_Unit_LTV: (FICO Score) / (LTV).\n\n#### Step C: Adjust the Cost Function\nThe 10:1 ratio might be too aggressive for this specific dataset distribution.\n\n##### Action: Retune the grid search with a 5:1 ratio or use a \"Profit Scorer\" (Interest Income from Good Loans minus Principal Loss from Defaults) instead of a pure \"Cost Scorer.\"\n\nfor the Next Step (Threshold Tuning & Interactions)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}